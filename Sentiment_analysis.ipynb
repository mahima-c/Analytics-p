{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJnjW6hVytwmQQ2NYEdJuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/deep-learning/blob/main/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n-h5EPf47jg"
      },
      "source": [
        "**Sentiment analysis**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNQp_WgI4-IC"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "max_len = 200\r\n",
        "n_words = 10000\r\n",
        "dim_embedding = 256\r\n",
        "EPOCHS = 20\r\n",
        "BATCH_SIZE = 500\r\n",
        "\r\n",
        "def load_data():\r\n",
        "        # Load data.\r\n",
        "        (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\r\n",
        "        # Pad sequences with max_len.\r\n",
        "        X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\r\n",
        "        X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\r\n",
        "        return (X_train, y_train), (X_test, y_test)\r\n",
        "\r\n",
        "def build_model():\r\n",
        "    model = models.Sequential()\r\n",
        "    # Input: - eEmbedding Layer.\r\n",
        "    # The model will take as input an integer matrix of size (batch,     # input_length).\r\n",
        "    # The model will output dimension (input_length, dim_embedding).\r\n",
        "    # The largest integer in the input should be no larger\r\n",
        "    # than n_words (vocabulary size).\r\n",
        "    model.add(layers.Embedding(n_words, dim_embedding, input_length=max_len))\r\n",
        "    model.add(layers.Dropout(0.3))\r\n",
        "    # Takes the maximum value of either feature vector from each of     # the n_words features.\r\n",
        "    model.add(layers.GlobalMaxPooling1D())\r\n",
        "    model.add(layers.Dense(128, activation='relu'))\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\r\n",
        "    return model\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ul_rSIv56Zn",
        "outputId": "f7c4eac0-d7c3-4531-8cad-a63ce9e77c3e"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = load_data()\r\n",
        "model = build_model()\r\n",
        "model.summary()\r\n",
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",\r\n",
        " metrics = [\"accuracy\"]\r\n",
        ")\r\n",
        "score = model.fit(X_train, y_train,\r\n",
        " epochs = EPOCHS,\r\n",
        " batch_size = BATCH_SIZE,\r\n",
        " validation_data = (X_test, y_test)\r\n",
        ")\r\n",
        "score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\r\n",
        "print(\"\\nTest score:\", score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 256)          2560000   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200, 256)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 2,593,025\n",
            "Trainable params: 2,593,025\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "50/50 [==============================] - 23s 453ms/step - loss: 0.6869 - accuracy: 0.5599 - val_loss: 0.6355 - val_accuracy: 0.8102\n",
            "Epoch 2/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.5422 - accuracy: 0.8200 - val_loss: 0.3680 - val_accuracy: 0.8598\n",
            "Epoch 3/20\n",
            "50/50 [==============================] - 22s 443ms/step - loss: 0.2977 - accuracy: 0.8811 - val_loss: 0.3038 - val_accuracy: 0.8747\n",
            "Epoch 4/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.2183 - accuracy: 0.9176 - val_loss: 0.2882 - val_accuracy: 0.8792\n",
            "Epoch 5/20\n",
            "50/50 [==============================] - 22s 443ms/step - loss: 0.1717 - accuracy: 0.9397 - val_loss: 0.2875 - val_accuracy: 0.8775\n",
            "Epoch 6/20\n",
            "50/50 [==============================] - 22s 443ms/step - loss: 0.1328 - accuracy: 0.9551 - val_loss: 0.2919 - val_accuracy: 0.8743\n",
            "Epoch 7/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.1010 - accuracy: 0.9698 - val_loss: 0.3043 - val_accuracy: 0.8686\n",
            "Epoch 8/20\n",
            "50/50 [==============================] - 22s 443ms/step - loss: 0.0743 - accuracy: 0.9797 - val_loss: 0.3222 - val_accuracy: 0.8632\n",
            "Epoch 9/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.0576 - accuracy: 0.9857 - val_loss: 0.3402 - val_accuracy: 0.8603\n",
            "Epoch 10/20\n",
            "50/50 [==============================] - 22s 444ms/step - loss: 0.0415 - accuracy: 0.9912 - val_loss: 0.3541 - val_accuracy: 0.8601\n",
            "Epoch 11/20\n",
            "50/50 [==============================] - 22s 444ms/step - loss: 0.0315 - accuracy: 0.9936 - val_loss: 0.3714 - val_accuracy: 0.8586\n",
            "Epoch 12/20\n",
            "50/50 [==============================] - 22s 446ms/step - loss: 0.0240 - accuracy: 0.9963 - val_loss: 0.3880 - val_accuracy: 0.8573\n",
            "Epoch 13/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.0179 - accuracy: 0.9975 - val_loss: 0.4029 - val_accuracy: 0.8562\n",
            "Epoch 14/20\n",
            "50/50 [==============================] - 22s 444ms/step - loss: 0.0143 - accuracy: 0.9976 - val_loss: 0.4181 - val_accuracy: 0.8548\n",
            "Epoch 15/20\n",
            "50/50 [==============================] - 22s 446ms/step - loss: 0.0118 - accuracy: 0.9979 - val_loss: 0.4346 - val_accuracy: 0.8532\n",
            "Epoch 16/20\n",
            "50/50 [==============================] - 22s 444ms/step - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.4473 - val_accuracy: 0.8531\n",
            "Epoch 17/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.0080 - accuracy: 0.9991 - val_loss: 0.4605 - val_accuracy: 0.8518\n",
            "Epoch 18/20\n",
            "50/50 [==============================] - 22s 443ms/step - loss: 0.0069 - accuracy: 0.9993 - val_loss: 0.4745 - val_accuracy: 0.8508\n",
            "Epoch 19/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.0062 - accuracy: 0.9990 - val_loss: 0.4824 - val_accuracy: 0.8518\n",
            "Epoch 20/20\n",
            "50/50 [==============================] - 22s 445ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.5032 - val_accuracy: 0.8484\n",
            "50/50 [==============================] - 2s 39ms/step - loss: 0.5032 - accuracy: 0.8484\n",
            "\n",
            "Test score: 0.5031759738922119\n",
            "Test accuracy: 0.848360002040863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a7DUMGo7POJ"
      },
      "source": [
        "**Predicting output**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-zq5L8756du"
      },
      "source": [
        "# Making predictions.\r\n",
        "\r\n",
        "# predictions = model.predict(X)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEX24g1N56hu",
        "outputId": "189a7e64-7e16-4593-d96f-3a4d65733afa"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1987,    2,   45,   55,  221,   15,  670, 5304,  526,   14, 1069,\n",
              "          4,  405,    5, 2438,    7,   27,   85,  108,  131,    4, 5045,\n",
              "       5304, 3884,  405,    9, 3523,  133,    5,   50,   13,  104,   51,\n",
              "         66,  166,   14,   22,  157,    9,    4,  530,  239,   34, 8463,\n",
              "       2801,   45,  407,   31,    7,   41, 3778,  105,   21,   59,  299,\n",
              "         12,   38,  950,    5, 4521,   15,   45,  629,  488, 2733,  127,\n",
              "          6,   52,  292,   17,    4, 6936,  185,  132, 1988, 5304, 1799,\n",
              "        488, 2693,   47,    6,  392,  173,    4,    2, 4378,  270, 2352,\n",
              "          4, 1500,    7,    4,   65,   55,   73,   11,  346,   14,   20,\n",
              "          9,    6,  976, 2078,    7, 5293,  861,    2,    5, 4182,   30,\n",
              "       3127,    2,   56,    4,  841,    5,  990,  692,    8,    4, 1669,\n",
              "        398,  229,   10,   10,   13, 2822,  670, 5304,   14,    9,   31,\n",
              "          7,   27,  111,  108,   15, 2033,   19, 7836, 1429,  875,  551,\n",
              "         14,   22,    9, 1193,   21,   45, 4829,    5,   45,  252,    8,\n",
              "          2,    6,  565,  921, 3639,   39,    4,  529,   48,   25,  181,\n",
              "          8,   67,   35, 1732,   22,   49,  238,   60,  135, 1162,   14,\n",
              "          9,  290,    4,   58,   10,   10,  472,   45,   55,  878,    8,\n",
              "        169,   11,  374, 5687,   25,  203,   28,    8,  818,   12,  125,\n",
              "          4, 3077], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}