{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNINByCXiffkBh3l2Ar4rkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/deep-learning/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8HhMV1oJCJX"
      },
      "source": [
        "Creating your own embedding using gensim\t\t\t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrEz8-6TJFmP"
      },
      "source": [
        "GloVe vectors trained on various large corpora (number of tokens ranging from 6 billion to 840 billion, vocabulary size from 400 thousand to 2.2 million) and of various dimensions (50, 100, 200, 300) are available from the GloVe project download page (https://nlp.stanford.edu/projects/glove/). It can be downloaded directly from the site, or using gensim or spaCy data downloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOjrWyO_Jidt"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CqhPs51JJd9",
        "outputId": "ccd55461-6738-4490-9559-a21e75578f42"
      },
      "source": [
        "import gensim.downloader as api\r\n",
        "from gensim.models import Word2Vec\r\n",
        "dataset = api.load(\"text8\")\r\n",
        "model = Word2Vec(dataset)\r\n",
        "model.save(\"data/text8-word2vec.bin\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8lTymbMJw1Y"
      },
      "source": [
        "gensim is an open source Python library designed to extract semantic meaning from text documents. One of its features is an excellent implementation of the Word2Vec algorithm, with an easy to use API that allows you to train and query your own Word2Vec model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RrY6p6jJxR1"
      },
      "source": [
        "Exploring the embedding space with gensim\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EOvwJUiJ519"
      },
      "source": [
        "Let us reload the Word2Vec model we just built and explore it using the gensim API. The actual word vectors can be accessed as a custom gensim class from the model's wv attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJfj5Z1uJ-cD"
      },
      "source": [
        "from gensim.models import KeyedVectors\r\n",
        "model = KeyedVectors.load(\"data/text8-word2vec.bin\")\r\n",
        "word_vectors = model.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhT6IzCIKmk4",
        "outputId": "795c7c53-2bc1-4626-ddaa-15f70adc2a49"
      },
      "source": [
        "words = word_vectors.vocab.keys()\r\n",
        "print([x for i, x in enumerate(words) if i < 10])\r\n",
        "assert(\"king\" in words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqojPXc4Kzen"
      },
      "source": [
        "We can look for similar words to a given word (\"king\"), shown as follows:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7wBYu6PKz1C",
        "outputId": "1809f7ce-bf06-4447-f372-3f01dab22ac1"
      },
      "source": [
        "def print_most_similar(word_conf_pairs, k):\r\n",
        "   for i, (word, conf) in enumerate(word_conf_pairs):\r\n",
        "       print(\"{:.3f} {:s}\".format(conf, word))\r\n",
        "       if i >= k-1:\r\n",
        "           break\r\n",
        "   if k < len(word_conf_pairs):\r\n",
        "       print(\"...\")\r\n",
        "print_most_similar(word_vectors.most_similar(\"king\"), 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.769 prince\n",
            "0.711 queen\n",
            "0.700 kings\n",
            "0.698 emperor\n",
            "0.689 throne\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPgDr3KGLQa2"
      },
      "source": [
        "The most_similar() method with a single parameter produces the following output. Here the floating point score is a measure of the similarity, higher values being better than lower values. As you can see, the similar words seem to be mostly accurate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udF6tWv0LIzm",
        "outputId": "62c81a6e-8cca-493c-f881-acb9cf97d464"
      },
      "source": [
        "word_vectors.most_similar(\"king\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prince', 0.7693274021148682),\n",
              " ('queen', 0.7108398675918579),\n",
              " ('kings', 0.7000777721405029),\n",
              " ('emperor', 0.6982017755508423),\n",
              " ('throne', 0.6885628700256348),\n",
              " ('sultan', 0.6810340881347656),\n",
              " ('constantine', 0.6770751476287842),\n",
              " ('pharaoh', 0.6768440008163452),\n",
              " ('darius', 0.6639723777770996),\n",
              " ('vii', 0.6617316603660583)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjcZ7NXkLgPd"
      },
      "source": [
        "You can also do vector arithmetic similar to the country-capital example we described earlier. Our objective is to see if the relation Paris : France :: Berlin : Germany holds true. This is equivalent to saying that the distance in embedding space between Paris and France should be the same as that between Berlin and Germany. In other words, France - Paris + Berlin should give us Germany. In code, then, this would translate to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GySipUOGLgoO",
        "outputId": "aa9361d8-907b-48dc-d74e-bc6fcf5afc6a"
      },
      "source": [
        "print_most_similar(word_vectors.most_similar(\r\n",
        "   positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.798 germany\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLtPZ9ZqLrJJ"
      },
      "source": [
        "The preceding similarity value reported is Cosine similarity, but a better measure of similarity was proposed by Levy and Goldberg [9] which is also implemented in the gensim API:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKB76XC2LrnS",
        "outputId": "bb2c4b3e-c05c-4fa0-bddc-e9f12dd5c4b2"
      },
      "source": [
        "print_most_similar(word_vectors.most_similar_cosmul(\r\n",
        "   positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.966 germany\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leCjZeFHL5Ex"
      },
      "source": [
        "gensim also provides a doesnt_match() function, which can be used to detect the odd one out of a list of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQZGnb-yL5kC",
        "outputId": "38f5ad5c-3c69-46b6-8858-2914362aebb2"
      },
      "source": [
        "print(word_vectors.doesnt_match([\"hindus\", \"parsis\", \"singapore\", \"christians\"]))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "singapore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5oU2atbMLYd"
      },
      "source": [
        "This gives us singapore as expected, since it is the only country among a set of words identifying religions.\r\n",
        "\r\n",
        "We can also calculate the similarity between two words. Here we demonstrate that the distance between related words is less than that of unrelated words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByGIEr_TMLv6",
        "outputId": "d4c5a3c9-ae35-4fdf-bf96-d2bb0abe3977"
      },
      "source": [
        "for word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\r\n",
        "   print(\"similarity({:s}, {:s}) = {:.3f}\".format(\r\n",
        "       \"man\", word,\r\n",
        "       word_vectors.similarity(\"man\", word)\r\n",
        "   ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "similarity(man, woman) = 0.736\n",
            "similarity(man, dog) = 0.456\n",
            "similarity(man, whale) = 0.266\n",
            "similarity(man, tree) = 0.316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxOz-0R0MjgU"
      },
      "source": [
        "The similar_by_word() function is functionally equivalent to similar() except that the latter normalizes the vector before comparing by default. There is also a related similar_by_vector() function which allows you to find similar words by specifying a vector as input. Here we try to find words that are similar to \"singapore\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gcS80ZpMj1p",
        "outputId": "36cd143b-17a8-4f7b-a3bf-66e84b9aade0"
      },
      "source": [
        "print(print_most_similar(\r\n",
        "   word_vectors.similar_by_word(\"singapore\"), 5)\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.886 malaysia\n",
            "0.852 indonesia\n",
            "0.844 bahamas\n",
            "0.823 brunei\n",
            "0.813 taiwan\n",
            "...\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgc3IBMYMtGv"
      },
      "source": [
        "We can also compute the distance between two words in the embedding space using the distance() function. This is really just 1 - similarity():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbquIjrsMtep",
        "outputId": "522e622b-13c8-4eb5-f8f4-7c7b67034f8d"
      },
      "source": [
        "print(\"distance(singapore, malaysia) = {:.3f}\".format(\r\n",
        "   word_vectors.distance(\"singapore\", \"malaysia\")\r\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance(singapore, malaysia) = 0.114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8TFilngM-Ar"
      },
      "source": [
        "We can also look up vectors for a vocabulary word either directly from the word_vectors object, or by using the word_vec() wrapper, shown as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycbG3h7nM8tQ"
      },
      "source": [
        "vec_song = word_vectors[\"song\"]\r\n",
        "vec_song_2 = word_vectors.word_vec(\"song\", use_norm=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ROcH4ELNCq4",
        "outputId": "52f70c52-5190-4722-f9fb-5c58c30026f1"
      },
      "source": [
        "vec_song,vec_song_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.3156664 ,  0.10720551, -3.144506  ,  1.0351552 , -0.5526515 ,\n",
              "         1.9685628 ,  0.7325977 ,  1.2272649 ,  0.02926658, -0.03596711,\n",
              "        -0.39086467, -1.527401  ,  2.4827774 , -0.13089536, -1.0787141 ,\n",
              "         1.3572806 ,  0.9153923 , -1.8670173 ,  0.2830431 , -2.4028175 ,\n",
              "        -0.12260415,  1.5906199 ,  2.0122545 , -0.27040014, -2.3553162 ,\n",
              "        -2.8955455 ,  3.6636055 ,  0.36103353,  0.9389921 ,  1.7202243 ,\n",
              "        -1.7266402 ,  0.18956617,  1.1643379 , -0.91071165,  1.4218795 ,\n",
              "        -0.7663593 , -2.126566  ,  2.51259   ,  2.4552925 ,  0.36232647,\n",
              "        -0.21931393,  0.5776023 , -1.430306  , -2.007075  ,  0.541459  ,\n",
              "        -0.91987723,  0.46336558,  2.3473291 ,  2.8090487 ,  2.637745  ,\n",
              "         0.10578953, -3.1830933 ,  0.6218483 , -2.4736285 ,  1.4344423 ,\n",
              "        -1.2269671 , -0.48875085,  0.21356896,  0.8835547 , -1.2243378 ,\n",
              "         1.6037246 ,  0.5887441 , -0.17149971,  2.3121934 ,  1.1105942 ,\n",
              "         2.9615018 ,  0.23544699, -1.2602105 , -0.10360697,  0.7481737 ,\n",
              "         0.03783132,  3.1161408 , -0.03203155,  0.940568  ,  1.4922196 ,\n",
              "         1.86886   , -0.58035266, -0.23771864, -0.3068951 ,  1.1082648 ,\n",
              "        -0.8294768 , -4.7897134 ,  2.8576577 , -0.4772139 ,  0.24661721,\n",
              "         0.9527859 , -0.5451346 , -0.4837801 ,  0.8650161 ,  1.1890062 ,\n",
              "        -1.2372638 ,  0.9790553 , -1.5771251 , -2.9140735 , -1.4389075 ,\n",
              "        -3.0109322 ,  0.5130417 , -1.5593301 ,  0.24650377, -0.00582224],\n",
              "       dtype=float32),\n",
              " array([-0.01978799,  0.00672033, -0.19711778,  0.06489015, -0.03464374,\n",
              "         0.12340213,  0.04592392,  0.07693283,  0.00183462, -0.00225465,\n",
              "        -0.0245019 , -0.09574728,  0.15563639, -0.00820536, -0.06762071,\n",
              "         0.08508305,  0.05738265, -0.1170366 ,  0.01774295, -0.15062399,\n",
              "        -0.00768561,  0.09971025,  0.126141  , -0.01695041, -0.14764631,\n",
              "        -0.18151134,  0.22965826,  0.02263189,  0.05886204,  0.10783467,\n",
              "        -0.10823687,  0.01188322,  0.07298815, -0.05708924,  0.08913252,\n",
              "        -0.04804031, -0.13330677,  0.15750523,  0.15391345,  0.02271294,\n",
              "        -0.013748  ,  0.03620781, -0.08966074, -0.12581632,  0.03394212,\n",
              "        -0.0576638 ,  0.02904672,  0.14714563,  0.17608917,  0.16535075,\n",
              "         0.00663157, -0.19953668,  0.03898143, -0.15506288,  0.08992003,\n",
              "        -0.07691415, -0.03063803,  0.01338787,  0.05538687, -0.07674934,\n",
              "         0.10053173,  0.03690625, -0.0107507 ,  0.14494309,  0.06961916,\n",
              "         0.18564591,  0.01475933, -0.07899807, -0.00649475,  0.04690032,\n",
              "         0.00237151,  0.19533966, -0.00200794,  0.05896083,  0.09354188,\n",
              "         0.11715212, -0.03638022, -0.01490173, -0.01923815,  0.06947314,\n",
              "        -0.05199692, -0.3002499 ,  0.17913629, -0.02991482,  0.01545955,\n",
              "         0.05972672, -0.03417253, -0.03032644,  0.05422475,  0.07453453,\n",
              "        -0.07755962,  0.06137346, -0.0988643 , -0.18267278, -0.09019994,\n",
              "        -0.18874452,  0.03216074, -0.0977488 ,  0.01545244, -0.00036497],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw22HGmvNnAW"
      },
      "source": [
        "**Our example is a spam detector that will classify Short Message Service (SMS) or text messages as either \"ham\" or \"spam.\"** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJKBCQI6NokB"
      },
      "source": [
        "import argparse\r\n",
        "import gensim.downloader as api\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDh0pk3QOAYB"
      },
      "source": [
        "Getting the data\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441fFtmcOAt7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbYLYj-IOGXe"
      },
      "source": [
        "The data for our model is available publicly and comes from the SMS spam collection dataset from the UCI Machine Learning Repository [11]. The following code will download the file and parse it to produce a list of SMS messages and their corresponding labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUxVf16-OHMC",
        "outputId": "33c17b6b-47a5-49e4-fb99-d3f625fd4a11"
      },
      "source": [
        "def download_and_read(url):\r\n",
        "   local_file = url.split('/')[-1]\r\n",
        "   p = tf.keras.utils.get_file(local_file, url,\r\n",
        "       extract=True, cache_dir=\".\")\r\n",
        "   labels, texts = [], []\r\n",
        "   local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\r\n",
        "   with open(local_file, \"r\") as fin:\r\n",
        "       for line in fin:\r\n",
        "           label, text = line.strip().split('\\t')\r\n",
        "           labels.append(1 if label == \"spam\" else 0)\r\n",
        "           texts.append(text)\r\n",
        "   return texts, labels\r\n",
        "DATASET_URL =  \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\r\n",
        "texts, labels = download_and_read(DATASET_URL)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0HSeT_nPawU"
      },
      "source": [
        "Making the data ready for use\r\n",
        "The next step is to process the data so it can be consumed by the network. The SMS text needs to be fed into the network as a sequence of integers, where each word is represented by its corresponding ID in the vocabulary. We will use the Keras tokenizer to convert each SMS text into a sequence of words, and then create the vocabulary using the fit_on_texts() method on the tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVNCIC57PpBI"
      },
      "source": [
        "We then convert the SMS messages to a sequence of integers using the texts_to_sequences(). Finally, since the network can only work with fixed length sequences of integers, we call the pad_sequences() function to pad the shorter SMS messages with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BSAtrSFPxGu"
      },
      "source": [
        "The longest SMS message in our dataset has 189 tokens (words). In many applications where there may be a few outlier sequences that are very long, we would restrict the length to a smaller number by setting the maxlen flag. In that case, sentences longer than maxlen tokens would be truncated, and sentences shorter than maxlen tokens would be padded:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPlC_CuwPbd9",
        "outputId": "df91a718-2f4a-47b0-c46b-2308b012ec5c"
      },
      "source": [
        "# tokenize and pad text\r\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts(texts)\r\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\r\n",
        "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    text_sequences)\r\n",
        "num_records = len(text_sequences)\r\n",
        "max_seqlen = len(text_sequences[0])\r\n",
        "print(\"{:d} sentences, max length: {:d}\".format(\r\n",
        "    num_records, max_seqlen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5574 sentences, max length: 189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ad7mygJQD2Z"
      },
      "source": [
        "We will also convert our labels to categorical or one-hot encoding format, because the loss function we would like to choose (categorical cross-entropy) expects to see the labels in that format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1SYIbYJQJVl"
      },
      "source": [
        "# labels\r\n",
        "NUM_CLASSES = 2\r\n",
        "cat_labels = tf.keras.utils.to_categorical(\r\n",
        "    labels, num_classes=NUM_CLASSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40S4OftkQUeF",
        "outputId": "50ed59d6-3153-4ca7-90b3-8676d0aa7d2f"
      },
      "source": [
        "# vocabulary\r\n",
        "word2idx = tokenizer.word_index\r\n",
        "idx2word = {v:k for k, v in word2idx.items()}\r\n",
        "word2idx[\"PAD\"] = 0\r\n",
        "idx2word[0] = \"PAD\"\r\n",
        "vocab_size = len(word2idx)\r\n",
        "print(\"vocab size: {:d}\".format(vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size: 9010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7O7Cl4tQEIG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um4-xFSyRAaC"
      },
      "source": [
        "# dataset\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (text_sequences, cat_labels))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = num_records // 4\r\n",
        "val_size = (num_records - test_size) // 10\r\n",
        "test_dataset = dataset.take(test_size)\r\n",
        "val_dataset = dataset.skip(test_size).take(val_size)\r\n",
        "train_dataset = dataset.skip(test_size + val_size)\r\n",
        "BATCH_SIZE = 128\r\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_pef11WRJCv"
      },
      "source": [
        "Building the embedding matrix\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "UM1rK91oRHp_",
        "outputId": "fe6dfafd-fb12-471f-ef31-7a3a3f25db37"
      },
      "source": [
        "# import gensim.downloader as api\r\n",
        "# api.info(\"models\").keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-856353e07d70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36minfo\u001b[0;34m(name, show_only_latest, name_only)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect model/corpus name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshow_only_latest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Incorrect model/corpus name"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnhxSlQ8TToK",
        "outputId": "ebe5b51f-bbb5-4696-88a6-f1b7fb018e02"
      },
      "source": [
        "def build_embedding_matrix(sequences, word2idx, embedding_dim,\r\n",
        "       embedding_file):\r\n",
        "   if os.path.exists(embedding_file):\r\n",
        "       E = np.load(embedding_file)\r\n",
        "   else:\r\n",
        "       vocab_size = len(word2idx)\r\n",
        "       E = np.zeros((vocab_size, embedding_dim))\r\n",
        "       word_vectors = api.load(EMBEDDING_MODEL)\r\n",
        "       for word, idx in word2idx.items():\r\n",
        "           try:\r\n",
        "               E[idx] = word_vectors.word_vec(word)\r\n",
        "           except KeyError:   # word not in embedding\r\n",
        "               pass\r\n",
        "       np.save(embedding_file, E)\r\n",
        "   return E\r\n",
        "EMBEDDING_DIM = 300\r\n",
        "DATA_DIR = \"data\"\r\n",
        "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\r\n",
        "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\r\n",
        "E = build_embedding_matrix(text_sequences, word2idx, \r\n",
        "   EMBEDDING_DIM,\r\n",
        "   EMBEDDING_NUMPY_FILE)\r\n",
        "print(\"Embedding matrix:\", E.shape)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[========------------------------------------------] 17.9% 67.2/376.1MB downloaded"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRVU7zU2Tvwn"
      },
      "source": [
        "Define the spam classifier\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpqYWYt3eKBn"
      },
      "source": [
        "The input is a sequence of integers. The first layer is an Embedding layer, which converts each input integer to a vector of size (embedding_dim). Depending on the run mode, that is, whether we will learn the embeddings from scratch, do transfer learning, or do fine-tuning, the Embedding layer in the network would be slightly different. When the network starts with randomly initialized embedding weights (run_mode == \"scratch\"), and learns the weights during the training, we set the trainable parameter to True. In the transfer learning case (run_mode == \"vectorizer\"), we set the weights from our embedding matrix E but set the trainable parameter to False, so it doesn't train. In the fine-tuning case (run_mode == \"finetuning\"), we set the embedding weights from our external matrix E, as well as set the layer to trainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54xTDAV4eYZN"
      },
      "source": [
        "Output of the embedding is fed into a convolutional layer. Here fixed size 3-token-wide 1D windows (kernel_size=3), also called time steps, are convolved against 256 random filters (num_filters=256) to produce vectors of size 256 for each time step. Thus, the output vector shape is (batch_size, time_steps, num_filters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBERFxJYefZM"
      },
      "source": [
        "Output of the convolutional layer is sent to a 1D spatial dropout layer. Spatial dropout will randomly drop entire feature maps output from the convolutional layer. This is a regularization technique to prevent over-fitting. This is then sent through a Global max pool layer, which takes the maximum value from each time step for each filter, resulting in a vector of shape (batch_size, num_filters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOM5GTHfegVf"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "5Y8taA_8eY9N",
        "outputId": "f0a2319e-75dd-4da7-e27d-96c475ce8ca7"
      },
      "source": [
        "class SpamClassifierModel(tf.keras.Model):\r\n",
        "   def __init__(self, vocab_sz, embed_sz, input_length,\r\n",
        "           num_filters, kernel_sz, output_sz,\r\n",
        "           run_mode, embedding_weights,\r\n",
        "           **kwargs):\r\n",
        "       super(SpamClassifierModel, self).__init__(**kwargs)\r\n",
        "       if run_mode == \"scratch\":\r\n",
        "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\r\n",
        "               embed_sz,\r\n",
        "               input_length=input_length,\r\n",
        "               trainable=True)\r\n",
        "       elif run_mode == \"vectorizer\":\r\n",
        "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\r\n",
        "               embed_sz,\r\n",
        "               input_length=input_length,\r\n",
        "               weights=[embedding_weights],\r\n",
        "               trainable=False)\r\n",
        "       else:\r\n",
        "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\r\n",
        "               embed_sz,\r\n",
        "               input_length=input_length,\r\n",
        "               weights=[embedding_weights],\r\n",
        "               trainable=True)\r\n",
        "       self.conv = tf.keras.layers.Conv1D(filters=num_filters,\r\n",
        "           kernel_size=kernel_sz,\r\n",
        "           activation=\"relu\")\r\n",
        "       self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\r\n",
        "       self.pool = tf.keras.layers.GlobalMaxPooling1D()\r\n",
        "       self.dense = tf.keras.layers.Dense(output_sz,\r\n",
        "           activation=\"softmax\")\r\n",
        "   def call(self, x):\r\n",
        "       x = self.embedding(x)\r\n",
        "       x = self.conv(x)\r\n",
        "       x = self.dropout(x)\r\n",
        "       x = self.pool(x)\r\n",
        "       x = self.dense(x)\r\n",
        "       return x\r\n",
        "# model definition\r\n",
        "conv_num_filters = 256\r\n",
        "conv_kernel_size = 3\r\n",
        "model = SpamClassifierModel(\r\n",
        "   vocab_size, EMBEDDING_DIM, max_seqlen,\r\n",
        "   conv_num_filters, conv_kernel_size, NUM_CLASSES,\r\n",
        "   run_mode, E)\r\n",
        "model.build(input_shape=(None, max_seqlen))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1bd2d99f5842>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSpamClassifierModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m    def __init__(self, vocab_sz, embed_sz, input_length,\n\u001b[1;32m      3\u001b[0m            \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m            \u001b[0mrun_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            **kwargs):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6UO3C-gTwPf"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW_90oqAfRFY"
      },
      "source": [
        "Train and evaluate the model\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3-JLW0fgHv3"
      },
      "source": [
        "One thing to notice is that the dataset is somewhat imbalanced, there are only 747 instances of spam, compared to 4827 instances of ham. The network could achieve close to 87% accuracy simply by always predicting the majority class. To alleviate this problem, we set class weights to indicate that an error on a spam SMS is 8 times as expensive as an error on a ham SMS. This is indicated by the CLASS_WEIGHTS variable, which is passed into the model.fit() call as an additional parameter.\r\n",
        "\r\n",
        "After training for 3 epochs, we evaluate the model against the test set, and report the accuracy and confusion matrix of the model against the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHJUvLWbgIW-"
      },
      "source": [
        "NUM_EPOCHS = 3\r\n",
        "# data distribution is 4827 ham and 747 spam (total 5574), which\r\n",
        "# works out to approx 87% ham and 13% spam, so we take reciprocals\r\n",
        "# and this works out to being each spam (1) item as being \r\n",
        "# approximately 8 times as important as each ham (0) message.\r\n",
        "CLASS_WEIGHTS = { 0: 1, 1: 8 }\r\n",
        "# train model\r\n",
        "model.fit(train_dataset, epochs=NUM_EPOCHS,\r\n",
        "   validation_data=val_dataset,\r\n",
        "   class_weight=CLASS_WEIGHTS)\r\n",
        "# evaluate against test set\r\n",
        "labels, predictions = [], []\r\n",
        "for Xtest, Ytest in test_dataset:\r\n",
        "   Ytest_ = model.predict_on_batch(Xtest)\r\n",
        "   ytest = np.argmax(Ytest, axis=1)\r\n",
        "   ytest_ = np.argmax(Ytest_, axis=1)\r\n",
        "   labels.extend(ytest.tolist())\r\n",
        "   predictions.extend(ytest.tolist())\r\n",
        "print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\r\n",
        "print(\"confusion matrix\")\r\n",
        "print(confusion_matrix(labels, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMqiVLHUTmTd",
        "outputId": "808806fc-cdc8-404e-c6fe-4652b6176246"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "module_url = \"https://tfhub.dev/google/elmo/2\"\r\n",
        "tf.compat.v1.disable_eager_execution()\r\n",
        "elmo = hub.Module(module_url, trainable=False)\r\n",
        "embeddings = elmo([\r\n",
        "       \"i like green eggs and ham\",\r\n",
        "       \"would you eat them in a box\"\r\n",
        "   ],\r\n",
        "   signature=\"default\",\r\n",
        "   as_dict=True\r\n",
        ")[\"elmo\"]\r\n",
        "print(embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(2, 7, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PF2Jho-jONT"
      },
      "source": [
        "Output is (2, 7, 1024). The first index tells us that our input contained 2 sentences. The second index refers to the maximum number of words across all sentences, in this case, 7. The model automatically pads the output to the longest sentence. The third index gives us the size of the contextual word embedding created by ELMo; each word is converted to a vector of size (1024)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "5DrWoFAbjNdB",
        "outputId": "a92ae916-989b-4921-c6ce-1f0c57de38c3"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/tf2-preview/elmo/2\"\r\n",
        "embed = hub.KerasLayer(module_url)\r\n",
        "embeddings = embed([\r\n",
        "   \"i like green eggs and ham\",\r\n",
        "   \"would you eat them in a box\"\r\n",
        "])[\"elmo\"]\r\n",
        "print(embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f0273dc44dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodule_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://tfhub.dev/google/tf2-preview/elmo/2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m embeddings = embed([\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m\"i like green eggs and ham\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m\"would you eat them in a box\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_training_argument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_has_training_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_hub_module_v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Expected before TF2.4.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m       \u001b[0mset_load_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodule_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_load_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected a string, got %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m   \u001b[0mmodule_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m   is_hub_module_v1 = tf.io.gfile.exists(\n\u001b[1;32m     94\u001b[0m       native_module.get_module_proto_path(module_path))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/module_v2.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/registry.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         logging.info(\"%s %s does not support the provided handle.\", self._name,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/uncompressed_module_resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mhandle_with_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_uncompressed_format_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mgcs_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_gcs_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_with_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_resolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgcs_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_append_uncompressed_format_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_hub/resolver.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, handle)\u001b[0m\n\u001b[1;32m    487\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: gs://tfhub-modules/google/tf2-preview/elmo/2/uncompressed does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djoDPHiqmZZY"
      },
      "source": [
        "The BERT model comes in two major flavorsBERT-base and BERT-large. BERT-base has 12 encoder layers, 768 hidden units, and 12 attention heads, with 110 million parameters in all. BERT-large has 24 encoder layers, 1024 hidden units, and 16 attention heads, with 340 million parameters. More details can be found in the BERT GitHub repository [34]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf2EE4ekmaDP"
      },
      "source": [
        "BERT Pretraining is a very expensive process and can currently only be achieved using Tensor Processing Units (TPUs), which are only available from Google via its Colab network [32] or Google Cloud Platform [33]. However, fine-tuning the BERT-base with custom datasets is usually achievable on GPU instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlowgQgdmjGz"
      },
      "source": [
        "Once the BERT model is fine-tuned for your domain, the embeddings from the last four hidden layers usually produce good results for downstream tasks. Which embedding or combination of embeddings (via summing, averaging, max-pooling, or concatenating) to use is usually based on the type of task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xq8BCrMmmlL"
      },
      "source": [
        "Using BERT as a feature extractor\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHXXQWhXmcb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAZxj8vPmctE"
      },
      "source": [
        ""
      ]
    }
  ]
}