{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN2DL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMuiyD4iG1xnrv1GiK8WI1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/deep-learning/blob/main/CNN2DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auj8kywwJYnh"
      },
      "source": [
        "\r\n",
        "**CLASSIFICATION AND LOCALIZATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D32ydMMPLFE5"
      },
      "source": [
        "In the classification and localization task not only do you have to report the class of object found in the image, but also the coordinates of the bounding box where the object appears in the image. This type of task assumes that there is only one instance of the object in an image.\r\n",
        "\r\n",
        "This can be achieved by attaching a \"regression head\" in addition to the \"classification head\" in a typical classification network. Recall that in a classification network, the final output of convolution and pooling operations, called the feature map, is fed into a fully connected network that produces a vector of class probabilities. This fully connected network is called the classification head, and it is tuned using a categorical loss function (Lc) such as categorical cross entropy.\r\n",
        "\r\n",
        "Similarly, a regression head is another fully connected network that takes the feature map and produces a vector (x, y, w, h) representing the top-left x and y coordinates, width and height of the bounding box. It is tuned using a continuous loss function (Lr) such as mean squared error. The entire network is tuned using a linear combination of the two losses, that is:\r\n",
        "\r\n",
        "\r\n",
        "Here  is a hyperparameter and can take a value between 0 and 1. Unless the value is determined by some domain knowledge about the problem, it can be set to 0.5.\r\n",
        "\r\n",
        "The following figure shows a typical classification and localization network architecture. As you can see, the only difference with respect to a typical CNN classification network is the additional regression head on the top right:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWR96yN7K_cp"
      },
      "source": [
        "**SEMANTIC SEGMENTATION**\r\n",
        "Another class of problem that builds on the basic classification idea is \"semantic segmentation.\" Here the aim is to classify every single pixel on the image as belonging to a single class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvxCfiKyMFjU"
      },
      "source": [
        "Another class of problem that builds on the basic classification idea is \"semantic segmentation.\" Here the aim is to classify every single pixel on the image as belonging to a single class.\r\n",
        "\r\n",
        "An initial method of implementation could be to build a classifier network for each pixel, where the input is a small neighborhood around each pixel. In practice, this approach is not very performant, so an improvement over this implementation might be to run the image through convolutions that will increase the feature depth, while keeping the image width and height constant. Each pixel then has a feature map that can be sent through a fully connected network that predicts the class of the pixel. However, in practice, this is also quite expensive, and it is not normally used.\r\n",
        "\r\n",
        "A third approach is to use a CNN encoder-decoder network, where the encoder decreases the width and height of the image but increases its depth (number of features), while the decoder uses transposed convolution operations to increase its size and decrease depth. Transpose convolution (or upsampling) is the process of going in the opposite direction of a normal convolution. The input to this network is the image and the output is the segmentation map.\r\n",
        "\r\n",
        "A popular implementation of this encoder-decoder architecture is the U-Net (a good implementation is available at: https://github.com/jakeret/tf_unet), originally developed for biomedical image segmentation, which has additional skip-connections between corresponding layers of the encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezBESxXRMGrc"
      },
      "source": [
        "**OBJECT DETECTION**\r\n",
        "The object detection task is similar to the classification and localization tasks. The big difference is that now there are multiple objects in the image, and for each one we need to find the class and bounding box coordinates. In addition, neither the number of objects nor their size is known in advance. As you can imagine, this is a difficult problem and a fair amount of research has gone into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8TOSqa6MYNz"
      },
      "source": [
        "A first approach to the problem might be to create many random crops of the input image and for each crop, apply the classification and localization networks we described earlier. However, such an approach is very wasteful in terms of computing and unlikely to be very successful.\r\n",
        "\r\n",
        "A more practical approach would be use a tool such as Selective Search (Selective Search for Object Recognition, by Uijlings et al, http://www.huppelen.nl/publications/selectiveSearchDraft.pdf), which uses traditional computer vision techniques to find areas in the image that might contain objects. These regions are called \"Region Proposals,\" and the network to detect them was called \"Region Proposal Network,\" or R-CNN. In the original R-CNN, the regions were resized and fed into a network to yield image vectors:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biMmH_plMYw5"
      },
      "source": [
        "\r\n",
        "Classifying Fashion-MNIST with a tf.keras - estimator model\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo_bMShfZuXF"
      },
      "source": [
        "import os\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "# How many categories we are predicting from (0-9)\r\n",
        "LABEL_DIMENSIONS = 10\r\n",
        "(train_images, train_labels), (test_images, test_labels) = \r\n",
        "    tf.keras.datasets.fashion_mnist.load_data()\r\n",
        "TRAINING_SIZE = len(train_images)\r\n",
        "TEST_SIZE = len(test_images)\r\n",
        "train_images = np.asarray(train_images, dtype=np.float32) / 255\r\n",
        "# Convert the train images and add channels\r\n",
        "train_images = train_images.reshape((TRAINING_SIZE, 28, 28, 1))\r\n",
        "test_images = np.asarray(test_images, dtype=np.float32) / 255\r\n",
        "# Convert the train images and add channels\r\n",
        "test_images = test_images.reshape((TEST_SIZE, 28, 28, 1))\r\n",
        "train_labels  = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\r\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\r\n",
        "# Cast the labels to float\r\n",
        "train_labels = train_labels.astype(np.float32)\r\n",
        "test_labels = test_labels.astype(np.float32)\r\n",
        "print (train_labels.shape)\r\n",
        "print (test_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUoFxz8oZtwB"
      },
      "source": [
        "Now let's build a convolutional model with the tf.keras functional API:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhc4lfrlZzp2"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(28,28,1))  \r\n",
        "x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)\r\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\r\n",
        "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\r\n",
        "x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2)(x)\r\n",
        "x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)\r\n",
        "x = tf.keras.layers.Flatten()(x)\r\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\r\n",
        "predictions = tf.keras.layers.Dense(LABEL_DIMENSIONS, activation='softmax')(x)\r\n",
        "model = tf.keras.Model(inputs=inputs, outputs=predictions)\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6BbYBmwaCOB"
      },
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\r\n",
        "model.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer=optimizer,\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP-MD9r_MY0E"
      },
      "source": [
        "Define a strategy, which is None for now because we run on CPUs first:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtqClo8uaKMD"
      },
      "source": [
        "strategy = None\r\n",
        "#strategy = tf.distribute.MirroredStrategy()\r\n",
        "config = tf.estimator.RunConfig(train_distribute=strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_phJJvfealDL"
      },
      "source": [
        "Now let's convert the tf.keras model into a convenient Estimator:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQ3EP-ZaKN4"
      },
      "source": [
        "estimator = tf.keras.estimator.model_to_estimator(model, config=config)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZScoTXjMY3h"
      },
      "source": [
        "The next step is to define input functions for training and for testing, which is pretty easy if we use tf.data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA33DmRXJZoW"
      },
      "source": [
        "def input_fn(images, labels, epochs, batch_size):\r\n",
        "    # Convert the inputs to a Dataset\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\r\n",
        "    # Shuffle, repeat, and batch the examples.\r\n",
        "    SHUFFLE_SIZE = 5000\r\n",
        "    dataset = dataset.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\r\n",
        "    dataset = dataset.prefetch(None)\r\n",
        "    # Return the dataset. \r\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoRyJe05azcs"
      },
      "source": [
        "We are ready to start the training with the following code:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBoICdeZa4j3"
      },
      "source": [
        "BATCH_SIZE = 512\r\n",
        "EPOCHS = 50\r\n",
        "estimator_train_result = estimator.train(input_fn=lambda:input_fn(train_images, train_labels,\r\n",
        "                 epochs=EPOCHS,\r\n",
        "                 batch_size=BATCH_SIZE))\r\n",
        "print(estimator_train_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycD4Y_n3a4n2"
      },
      "source": [
        "estimator.evaluate(lambda:input_fn(test_images, \r\n",
        "                                   test_labels,\r\n",
        "                                   epochs=1,\r\n",
        "                                   batch_size=BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DQq302VbJiW"
      },
      "source": [
        "Run Fashion-MNIST the tf.keras - estimator model on GPUs\r\n",
        "In this section we aim at running the estimator on GPUs. All we need to do is to change the strategy into a MirroredStrategy(). This strategy uses one replica per device and sync replication for its multi-GPU version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUlYdfQGbJ4K"
      },
      "source": [
        "BATCH_SIZE = 512\r\n",
        "EPOCHS = 50\r\n",
        "estimator_train_result = estimator.train(input_fn=lambda:input_fn(train_images, train_labels,\r\n",
        "                 epochs=EPOCHS,\r\n",
        "                 batch_size=BATCH_SIZE))\r\n",
        "print(estimator_train_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmvzV_ZabS02"
      },
      "source": [
        "estimator.evaluate(lambda:input_fn(test_images, \r\n",
        "                                   test_labels,\r\n",
        "                                   epochs=1,\r\n",
        "                                   batch_size=BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMtDx6IBcF2I"
      },
      "source": [
        "Answering questions about images (VQA)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DNeTnzccGLC"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers, models\r\n",
        "# IMAGE\r\n",
        "#\r\n",
        "# Define CNN for visual processing\r\n",
        "cnn_model = models.Sequential()\r\n",
        "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\r\n",
        "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\r\n",
        "cnn_model.add(layers.MaxPooling2D(2, 2))\r\n",
        "cnn_model.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\r\n",
        "cnn_model.add(layers.Conv2D(128, (3, 3), activation='relu'))\r\n",
        "cnn_model.add(layers.MaxPooling2D(2, 2))\r\n",
        "cnn_model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\r\n",
        "cnn_model.add(layers.Conv2D(256, (3, 3), activation='relu'))\r\n",
        "cnn_model.add(layers.Conv2D(256, (3, 3), activation='relu'))\r\n",
        "cnn_model.add(layers.MaxPooling2D(2, 2))\r\n",
        "cnn_model.add(layers.Flatten())\r\n",
        "cnn_model.summary()\r\n",
        "# define the visual_model with proper input\r\n",
        "image_input = layers.Input(shape=(224, 224, 3))\r\n",
        "visual_model = cnn_model(image_input)\r\n",
        "Text can be encoded with an RNN â€“"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IL4gLMhcGPv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfYMkbXtcGRR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}