{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKNCE9QVx/401Xnc0y5CN6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/deep-learning/blob/main/DL_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq6h2q4m7Miw"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\r\n",
        "# network and training\r\n",
        "EPOCHS = 5\r\n",
        "BATCH_SIZE = 128\r\n",
        "VERBOSE = 1\r\n",
        "OPTIMIZER = tf.keras.optimizers.Adam()\r\n",
        "VALIDATION_SPLIT=0.95\r\n",
        "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\r\n",
        "INPUT_SHAPE = (IMG_ROWS, IMG_COLS, 1)\r\n",
        "NB_CLASSES = 10  # number of outputs = number of digits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpQxkSiKAegA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYAOYZyB7PmU"
      },
      "source": [
        "Then we define the LeNet network:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_5WgERM7Tot"
      },
      "source": [
        "#define the convnet \r\n",
        "def build(input_shape, classes):\r\n",
        "    model = models.Sequential()\r\n",
        "    # CONV => RELU => POOL\r\n",
        "    model.add(layers.Convolution2D(20, (5, 5), activation='relu', input_shape=input_shape))\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "    # Increasing the number of filters in deeper layers is a common technique used in deep learning:\r\n",
        "    # CONV => RELU => POOL\r\n",
        "    model.add(layers.Convolution2D(50, (5, 5), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\r\n",
        "    # Flatten => RELU layers\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(500, activation='relu'))\r\n",
        "    # a softmax classifier\r\n",
        "    model.add(layers.Dense(classes, activation=\"softmax\"))\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ1dhxSb8O6t",
        "outputId": "e0e4da15-2396-4339-80ba-9e571cac4429"
      },
      "source": [
        "# data: shuffled and split between train and test sets\r\n",
        "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\r\n",
        "# reshape\r\n",
        "X_train = X_train.reshape((60000, 28, 28, 1))\r\n",
        "X_test = X_test.reshape((10000, 28, 28, 1))\r\n",
        "# normalize\r\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\r\n",
        "# cast\r\n",
        "X_train = X_train.astype('float32')\r\n",
        "X_test = X_test.astype('float32')\r\n",
        "# convert class vectors to binary class matrices\r\n",
        "y_train = tf.keras.utils.to_categorical(y_train, NB_CLASSES)\r\n",
        "y_test = tf.keras.utils.to_categorical(y_test, NB_CLASSES)\r\n",
        "# initialize the optimizer and model\r\n",
        "model = build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\r\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER,\r\n",
        "              metrics=[\"accuracy\"])\r\n",
        "model.summary()\r\n",
        "# use TensorBoard, princess Aurora!\r\n",
        "callbacks = [\r\n",
        "  # Write TensorBoard logs to './logs' directory\r\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\r\n",
        "]\r\n",
        "# fit \r\n",
        "history = model.fit(X_train, y_train, \r\n",
        "                  batch_size=BATCH_SIZE, epochs=EPOCHS, \r\n",
        "                  verbose=VERBOSE, validation_split=VALIDATION_SPLIT,\r\n",
        "                  callbacks=callbacks)\r\n",
        "score = model.evaluate(X_test, y_test, verbose=VERBOSE)\r\n",
        "print(\"\\nTest score:\", score[0])\r\n",
        "print('Test accuracy:', score[1])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 24, 24, 20)        520       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 20)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 8, 8, 50)          25050     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 50)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 500)               400500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 431,080\n",
            "Trainable params: 431,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 9s 75ms/step - loss: 1.7713 - accuracy: 0.4402 - val_loss: 0.4902 - val_accuracy: 0.8527\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 1s 57ms/step - loss: 0.3917 - accuracy: 0.8719 - val_loss: 0.3045 - val_accuracy: 0.9053\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 1s 57ms/step - loss: 0.2224 - accuracy: 0.9397 - val_loss: 0.2074 - val_accuracy: 0.9385\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 1s 56ms/step - loss: 0.1632 - accuracy: 0.9533 - val_loss: 0.2194 - val_accuracy: 0.9331\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 1s 57ms/step - loss: 0.1256 - accuracy: 0.9665 - val_loss: 0.1639 - val_accuracy: 0.9478\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1401 - accuracy: 0.9557\n",
            "\n",
            "Test score: 0.14011676609516144\n",
            "Test accuracy: 0.9556999802589417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VwmHjfAgAt"
      },
      "source": [
        "Recognizing CIFAR-10 images with deep learning\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aicwoGXQAjPg"
      },
      "source": [
        "The CIFAR-10 dataset contains 60,000 color images of 32Ã—32 pixels in 3 channels, divided in 10 classes. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets provides 10,000 images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr5gVwAZAmMW"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models, optimizers\r\n",
        "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\r\n",
        "IMG_CHANNELS = 3\r\n",
        "IMG_ROWS = 32\r\n",
        "IMG_COLS = 32\r\n",
        "# constant\r\n",
        "BATCH_SIZE = 128\r\n",
        "EPOCHS = 20\r\n",
        "CLASSES = 10\r\n",
        "VERBOSE = 1\r\n",
        "VALIDATION_SPLIT = 0.2\r\n",
        "OPTIM = tf.keras.optimizers.RMSprop()\r\n",
        "INPUT_SHAPE=(IMG_ROWS,IMG_COLS,3)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GraUQWdpApWG"
      },
      "source": [
        "# define the convnet \r\n",
        "def build(input_shape, classes):\r\n",
        "    model = models.Sequential() \r\n",
        "    model.add(layers.Convolution2D(32, (3, 3), activation='relu',\r\n",
        "              input_shape=input_shape))\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\r\n",
        "    model.add(layers.Dropout(0.25))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(512, activation='relu'))\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(classes, activation='softmax'))\r\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "cKr1CIk0ApYQ",
        "outputId": "c900c13e-4205-4a82-c5ec-abda65e358ea"
      },
      "source": [
        "# use TensorBoard, princess Aurora!\r\n",
        "model = build(input_shape=INPUT_SHAPE, classes=CLASSES)\r\n",
        "callbacks = [\r\n",
        "  # Write TensorBoard logs to './logs' directory\r\n",
        "  tf.keras.callbacks.TensorBoard(log_dir='./logs')\r\n",
        "]\r\n",
        "# train\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=OPTIM,metrics=['accuracy'])\r\n",
        "model.fit(X_train, y_train, \r\n",
        "          batch_size=BATCH_SIZE,\r\n",
        "          epochs=EPOCHS, \r\n",
        "          validation_split=VALIDATION_SPLIT, \r\n",
        "          verbose=VERBOSE, \r\n",
        "          callbacks=callbacks) \r\n",
        "score = model.evaluate(X_test, y_test,\r\n",
        "                       batch_size=BATCH_SIZE, verbose=VERBOSE)\r\n",
        "print(\"\\nTest score:\", score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7d395a31aad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPTIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m model.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m     10\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utu_1cabC9kT"
      },
      "source": [
        "Improving the CIFAR-10 performance with a deeper network\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSQjpABEDE55"
      },
      "source": [
        "One way to improve the performance is to define a deeper network with multiple convolutional operations. In the following example we have a sequence of modules:\r\n",
        "\r\n",
        "1st module: (CONV+CONV+MaxPool+DropOut)\r\n",
        "\r\n",
        "2nd: module: (CONV+CONV+MaxPool+DropOut)\r\n",
        "\r\n",
        "3rd module: (CONV+CONV+MaxPool+DropOut)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuKIFiTIC-Hb"
      },
      "source": [
        "def build_model():\r\n",
        "    model = models.Sequential()\r\n",
        "    \r\n",
        "    # 1st block\r\n",
        "    model.add(layers.Conv2D(32, (3,3), padding='same', \r\n",
        "        input_shape=x_train.shape[1:], activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\r\n",
        "    model.add(layers.Dropout(0.2))\r\n",
        "    # 2nd block\r\n",
        "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\r\n",
        "    model.add(layers.Dropout(0.3))\r\n",
        "    # 3d block \r\n",
        "    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\r\n",
        "    model.add(layers.BatchNormalization())\r\n",
        "    model.add(layers.MaxPooling2D(pool_size=(2,2)))\r\n",
        "    model.add(layers.Dropout(0.4))\r\n",
        "    # dense  \r\n",
        "    model.add(layers.Flatten())\r\n",
        "    model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\r\n",
        "    return model\r\n",
        "    model.summary()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YAhoxbxApbm",
        "outputId": "11220bdb-8fba-450d-eac0-b1e7ca85b1f6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import datasets, layers, models, regularizers, optimizers\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "import numpy as np\r\n",
        " \r\n",
        "EPOCHS=50\r\n",
        "NUM_CLASSES = 10\r\n",
        "def load_data():\r\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\r\n",
        "    x_train = x_train.astype('float32')\r\n",
        "    x_test = x_test.astype('float32')\r\n",
        " \r\n",
        "    # normalize \r\n",
        "    mean = np.mean(x_train,axis=(0,1,2,3))\r\n",
        "    std = np.std(x_train,axis=(0,1,2,3))\r\n",
        "    x_train = (x_train-mean)/(std+1e-7)\r\n",
        "    x_test = (x_test-mean)/(std+1e-7)\r\n",
        " \r\n",
        "    y_train =  tf.keras.utils.to_categorical(y_train,NUM_CLASSES)\r\n",
        "    y_test =  tf.keras.utils.to_categorical(y_test,NUM_CLASSES)\r\n",
        "    return x_train, y_train, x_test, y_test\r\n",
        "# Then we need to have a part to train the network:\r\n",
        "(x_train, y_train, x_test, y_test) = load_data()\r\n",
        "model = build_model()\r\n",
        "model.compile(loss='categorical_crossentropy', \r\n",
        "              optimizer='RMSprop', \r\n",
        "              metrics=['accuracy'])\r\n",
        "# train\r\n",
        "batch_size = 64\r\n",
        "model.fit(x_train, y_train, batch_size=batch_size,\r\n",
        "          epochs=EPOCHS, validation_data=(x_test,y_test)) \r\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\r\n",
        "print(\"\\nTest score:\", score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "Epoch 1/50\n",
            "782/782 [==============================] - 32s 31ms/step - loss: 2.2396 - accuracy: 0.3781 - val_loss: 1.3893 - val_accuracy: 0.5658\n",
            "Epoch 2/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 1.1719 - accuracy: 0.6264 - val_loss: 1.0026 - val_accuracy: 0.6805\n",
            "Epoch 3/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.9059 - accuracy: 0.7069 - val_loss: 1.0527 - val_accuracy: 0.7026\n",
            "Epoch 4/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.7537 - accuracy: 0.7461 - val_loss: 0.6957 - val_accuracy: 0.7625\n",
            "Epoch 5/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 0.6532 - accuracy: 0.7794 - val_loss: 0.6630 - val_accuracy: 0.7812\n",
            "Epoch 6/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.5960 - accuracy: 0.7962 - val_loss: 0.6911 - val_accuracy: 0.7653\n",
            "Epoch 7/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.5449 - accuracy: 0.8122 - val_loss: 0.6133 - val_accuracy: 0.7963\n",
            "Epoch 8/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 0.5019 - accuracy: 0.8230 - val_loss: 0.6323 - val_accuracy: 0.7916\n",
            "Epoch 9/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.4605 - accuracy: 0.8395 - val_loss: 0.5118 - val_accuracy: 0.8245\n",
            "Epoch 10/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.4387 - accuracy: 0.8465 - val_loss: 0.5213 - val_accuracy: 0.8219\n",
            "Epoch 11/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.4104 - accuracy: 0.8576 - val_loss: 0.5100 - val_accuracy: 0.8260\n",
            "Epoch 12/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.3885 - accuracy: 0.8656 - val_loss: 0.4978 - val_accuracy: 0.8331\n",
            "Epoch 13/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.3599 - accuracy: 0.8747 - val_loss: 0.5230 - val_accuracy: 0.8310\n",
            "Epoch 14/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.3462 - accuracy: 0.8784 - val_loss: 0.4959 - val_accuracy: 0.8327\n",
            "Epoch 15/50\n",
            "782/782 [==============================] - 23s 30ms/step - loss: 0.3263 - accuracy: 0.8851 - val_loss: 0.4948 - val_accuracy: 0.8398\n",
            "Epoch 16/50\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.3270 - accuracy: 0.8844 - val_loss: 0.4965 - val_accuracy: 0.8415\n",
            "Epoch 17/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.3040 - accuracy: 0.8909 - val_loss: 0.5135 - val_accuracy: 0.8323\n",
            "Epoch 18/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2903 - accuracy: 0.8986 - val_loss: 0.4860 - val_accuracy: 0.8441\n",
            "Epoch 19/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2816 - accuracy: 0.9006 - val_loss: 0.4764 - val_accuracy: 0.8497\n",
            "Epoch 20/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2754 - accuracy: 0.9023 - val_loss: 0.4901 - val_accuracy: 0.8420\n",
            "Epoch 21/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2583 - accuracy: 0.9098 - val_loss: 0.4657 - val_accuracy: 0.8507\n",
            "Epoch 22/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2609 - accuracy: 0.9092 - val_loss: 0.5112 - val_accuracy: 0.8403\n",
            "Epoch 23/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.2536 - accuracy: 0.9103 - val_loss: 0.4753 - val_accuracy: 0.8494\n",
            "Epoch 24/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.2443 - accuracy: 0.9127 - val_loss: 0.4839 - val_accuracy: 0.8524\n",
            "Epoch 25/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.2366 - accuracy: 0.9154 - val_loss: 0.4767 - val_accuracy: 0.8550\n",
            "Epoch 26/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2316 - accuracy: 0.9183 - val_loss: 0.4838 - val_accuracy: 0.8541\n",
            "Epoch 27/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.2237 - accuracy: 0.9202 - val_loss: 0.4763 - val_accuracy: 0.8526\n",
            "Epoch 28/50\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.2182 - accuracy: 0.9205 - val_loss: 0.4793 - val_accuracy: 0.8504\n",
            "Epoch 29/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2070 - accuracy: 0.9265 - val_loss: 0.4775 - val_accuracy: 0.8551\n",
            "Epoch 30/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2057 - accuracy: 0.9267 - val_loss: 0.5221 - val_accuracy: 0.8489\n",
            "Epoch 31/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2029 - accuracy: 0.9280 - val_loss: 0.4859 - val_accuracy: 0.8579\n",
            "Epoch 32/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.1996 - accuracy: 0.9280 - val_loss: 0.4694 - val_accuracy: 0.8561\n",
            "Epoch 33/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.1907 - accuracy: 0.9303 - val_loss: 0.4832 - val_accuracy: 0.8575\n",
            "Epoch 34/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.1915 - accuracy: 0.9326 - val_loss: 0.4834 - val_accuracy: 0.8601\n",
            "Epoch 35/50\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.1833 - accuracy: 0.9352 - val_loss: 0.4684 - val_accuracy: 0.8555\n",
            "Epoch 36/50\n",
            "385/782 [=============>................] - ETA: 11s - loss: 0.1797 - accuracy: 0.9386"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaDYT1cYDxDi"
      },
      "source": [
        "Improving the CIFAR-10 performance with data augmentation\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OllCXyLDApeK"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "#image augmentation\r\n",
        "datagen = ImageDataGenerator(\r\n",
        "    rotation_range=30,\r\n",
        "    width_shift_range=0.2,\r\n",
        "    height_shift_range=0.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    )\r\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY-5HRMdEbAD"
      },
      "source": [
        "Now we can apply this intuition directly for training. Using the same CNN defined before, we simply generate more augmented images and then we train. For efficiency, the generator runs in parallel to the model. This allows image augmentation on a CPU while training in parallel on a GPU. Here is the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io6IIO-nEe8K"
      },
      "source": [
        "# train\r\n",
        "batch_size = 64\r\n",
        "model.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    verbose=1,validation_data=(x_test,y_test))\r\n",
        "# save to disk\r\n",
        "model_json = model.to_json()\r\n",
        "with open('model.json', 'w') as json_file:\r\n",
        "    json_file.write(model_json)\r\n",
        "model.save_weights('model.h5') \r\n",
        "# test\r\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\r\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhupqc2RE7dq"
      },
      "source": [
        "Predicting with CIFAR-10\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqb4MYeXF6JO"
      },
      "source": [
        "Let's suppose that we want to use the deep learning model we just trained for CIFAR-10 for a bulk evaluation of images.\r\n",
        "\r\n",
        "Since we saved the model and the weights, we do not need to train each time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B6zxRrdEe-o"
      },
      "source": [
        "import numpy as np\r\n",
        "from skimage.transform import resize\r\n",
        "from imageio import imread\r\n",
        "from tensorflow.keras.models import model_from_json\r\n",
        "from tensorflow.keras.optimizers import SGD\r\n",
        "# load model\r\n",
        "model_architecture = 'cifar10_architecture.json'\r\n",
        "model_weights = 'cifar10_weights.h5'\r\n",
        "model = model_from_json(open(model_architecture).read())\r\n",
        "model.load_weights(model_weights)\r\n",
        "# load images\r\n",
        "img_names = ['cat-standing.jpg', 'dog.jpg']\r\n",
        "imgs = [resize(imread(img_name), (32, 32)).astype(\"float32\") for img_name in img_names]\r\n",
        "imgs = np.array(imgs) / 255\r\n",
        "print(\"imgs.shape:\", imgs.shape)\r\n",
        "# train\r\n",
        "optim = SGD()\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optim,\r\n",
        "              metrics=['accuracy'])\r\n",
        "# predict \r\n",
        "predictions = model.predict_classes(imgs)\r\n",
        "print(\"predictions:\", predictions)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVbSAxnkF4bo"
      },
      "source": [
        "The weights learned by the model implemented in Caffe have been directly converted (https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3) in tf.keras and can be used by preloading them into the tf.keras model, which is implemented as follows, as described in the paper:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qll7_6zrEfCF"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import layers, models\r\n",
        "# define a VGG16 network\r\n",
        "def VGG_16(weights_path=None):\r\n",
        "    model = models.Sequential()\r\n",
        "    model.add(layers.ZeroPadding2D((1,1),input_shape=(224,224, 3)))\r\n",
        "    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(64, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(128, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(128, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(256, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.ZeroPadding2D((1,1)))\r\n",
        "    model.add(layers.Convolution2D(512, (3, 3), activation='relu'))\r\n",
        "    model.add(layers.MaxPooling2D((2,2), strides=(2,2)))\r\n",
        "    model.add(layers.Flatten())\r\n",
        "    #top layer of the VGG net\r\n",
        "    model.add(layers.Dense(4096, activation='relu'))\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(4096, activation='relu'))\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "    model.add(layers.Dense(1000, activation='softmax'))\r\n",
        "    if weights_path:\r\n",
        "        model.load_weights(weights_path)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw1bJpEfGpIy"
      },
      "source": [
        "Note that we are going to use predefined weights:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDemksH9Gpm4"
      },
      "source": [
        "import cv2\r\n",
        "im = cv2.resize(cv2.imread('cat.jpg'), (224, 224).astype(np.float32))\r\n",
        "#im = im.transpose((2,0,1))\r\n",
        "im = np.expand_dims(im, axis=0)\r\n",
        "# Test pretrained model\r\n",
        "path_file = os.path.join(os.path.expanduser(\"~\"), '.keras/models/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\r\n",
        "model = VGG_16(path_file)\r\n",
        "model.summary()\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\r\n",
        "out = model.predict(im)\r\n",
        "print(np.argmax(out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY2VF1VQHAj8"
      },
      "source": [
        "Utilizing tf.keras built-in VGG16 Net module\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-ISsGiSHBR6"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "# prebuild model with pre-trained weights on imagenet\r\n",
        "model = VGG16(weights='imagenet', include_top=True)\r\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy')\r\n",
        "# resize into VGG16 trained images' format\r\n",
        "im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))\r\n",
        "im = np.expand_dims(im, axis=0)\r\n",
        "im.astype(np.float32)\r\n",
        "# predict\r\n",
        "out = model.predict(im)\r\n",
        "index = np.argmax(out)\r\n",
        "print(index)\r\n",
        "plt.plot(out.ravel())\r\n",
        "plt.show()\r\n",
        "# this should print 820 for steaming train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-9_TPHUHmBJ"
      },
      "source": [
        "Recycling prebuilt deep learning models for extracting features\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFyU_oFbHiv0"
      },
      "source": [
        "One very simple idea is to use VGG16, and more generally DCNN, for feature extraction. This code implements the idea by extracting features from a specific layer. Note that we need to switch to the functional API since the sequential model only accepts layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJYB2b4HjFa"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 \r\n",
        "from tensorflow.keras import models\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "# prebuild model with pre-trained weights on imagenet\r\n",
        "base_model = VGG16(weights='imagenet', include_top=True)\r\n",
        "print (base_model)\r\n",
        "for i, layer in enumerate(base_model.layers):\r\n",
        "    print (i, layer.name, layer.output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vzYK-tyH_vW"
      },
      "source": [
        "# extract features from block4_pool block\r\n",
        "model = models.Model(inputs=base_model.input, \r\n",
        "    outputs=base_model.get_layer('block4_pool').output)\r\n",
        "img_path = 'cat.jpg'\r\n",
        "img = image.load_img(img_path, target_size=(224, 224))\r\n",
        "x = image.img_to_array(img)\r\n",
        "x = np.expand_dims(x, axis=0)\r\n",
        "x = preprocess_input(x)\r\n",
        "# get the features from this block\r\n",
        "features = model.predict(x)\r\n",
        "print(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QozWwPc7IOjr"
      },
      "source": [
        "You might wonder why we want to extract the features from an intermediate layer in a DCNN. The reasoning is that as the network learns to classify images into categories, each layer learns to identify the features that are necessary to perform the final classification. Lower layers identify lower-order features such as color and edges, and higher layers compose these lower-order features into higher-order features such as shapes or objects. Hence, the intermediate layer has the capability to extract important features from an image, and these features are more likely to help in different kinds of classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vf9BubRKIO3r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}