{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq without attention for machine translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMcbSMP7X+IKhWcVb1GhMNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima-c/deep-learning/blob/main/seq2seq_without_attention_for_machine_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr8XUflVWIBt"
      },
      "source": [
        "seq2seq without attention for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_dFNHVHWUMq"
      },
      "source": [
        "To understand the seq2seq model in greater detail, we will look at an example of one that learns how to translate from English to French using the French-English bilingual dataset from the Tatoeba Project (1997-2019) [26]. The dataset contains approximately 167,000 sentence pairs. To make our training go faster, we will only consider the first 30,000 sentence pairs for our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkcA_JWrWA-s"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import re\r\n",
        "import shutil\r\n",
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import unicodedata\r\n",
        "import zipfile\r\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMDBA3DnWyH8"
      },
      "source": [
        "If you recall the structure of the seq2seq network, the input to the encoder is a sequence of English words. On the decoder side, the input is a set of French words, and the output is the sequence of French words offset by 1 timestep. The following function will download the zip file, expand it, and create the datasets described before.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtyYXD0XHUg"
      },
      "source": [
        "The input is preprocessed to \"asciify\" the characters, separate out specific punctuations from their neighboring word, and remove all characters other than alphabets and these specific punctuation symbols. Finally, the sentences are converted to lowercase. Each English sentence is just converted to a single sequence of words. Each French sentence is converted into two sequences, one preceded by the BOS pseudo-word and the other followed by the end of sentence (EOS) pseudo-word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J77Z0HVNXHu_"
      },
      "source": [
        "def preprocess_sentence(sent):\r\n",
        "    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent) \r\n",
        "        if unicodedata.category(c) != \"Mn\"])\r\n",
        "    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\r\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\r\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\r\n",
        "    sent = sent.lower()\r\n",
        "    return sent\r\n",
        "\r\n",
        "\r\n",
        "def download_and_read(url, num_sent_pairs=30000):\r\n",
        "    local_file = url.split('/')[-1]\r\n",
        "    if not os.path.exists(local_file):\r\n",
        "        os.system(\"wget -O {:s} {:s}\".format(local_file, url))\r\n",
        "        with zipfile.ZipFile(local_file, \"r\") as zip_ref:\r\n",
        "            zip_ref.extractall(\".\")\r\n",
        "    local_file = os.path.join(\".\", \"fra.txt\")\r\n",
        "    en_sents, fr_sents_in, fr_sents_out = [], [], []\r\n",
        "    with open(local_file, \"r\") as fin:\r\n",
        "        for i, line in enumerate(fin):\r\n",
        "            en_sent, fr_sent,_ = line.strip().split('\\t')\r\n",
        "            en_sent = [w for w in preprocess_sentence(en_sent).split()]\r\n",
        "            fr_sent = preprocess_sentence(fr_sent)\r\n",
        "            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\r\n",
        "            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\r\n",
        "            en_sents.append(en_sent)\r\n",
        "            fr_sents_in.append(fr_sent_in)\r\n",
        "            fr_sents_out.append(fr_sent_out)\r\n",
        "            if i >= num_sent_pairs - 1:\r\n",
        "                break\r\n",
        "    return en_sents, fr_sents_in, fr_sents_out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L1ICzH4LiZ6"
      },
      "source": [
        "def clean_up_logs(data_dir):\r\n",
        "    checkpoint_dir = os.path.join(data_dir, \"checkpoints\")\r\n",
        "    if os.path.exists(checkpoint_dir):\r\n",
        "        shutil.rmtree(checkpoint_dir, ignore_errors=True)\r\n",
        "        os.makedirs(checkpoint_dir)\r\n",
        "    return checkpoint_dir"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L60h3AZWZVq6"
      },
      "source": [
        "NUM_SENT_PAIRS = 30000\r\n",
        "EMBEDDING_DIM = 256\r\n",
        "ENCODER_DIM, DECODER_DIM = 1024, 1024\r\n",
        "BATCH_SIZE = 64\r\n",
        "NUM_EPOCHS = 30\r\n",
        "\r\n",
        "tf.random.set_seed(42)\r\n",
        "\r\n",
        "data_dir = \"./data\"\r\n",
        "checkpoint_dir = clean_up_logs(data_dir)\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeGldrK26MGT"
      },
      "source": [
        "# data preparation\r\n",
        "download_url = \"http://www.manythings.org/anki/fra-eng.zip\"\r\n",
        "sents_en, sents_fr_in, sents_fr_out = download_and_read(download_url)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWq6Q-6qY7NP"
      },
      "source": [
        "Our next step is to tokenize our inputs and create the vocabulary. Since we have sequences in two different languages, we will create two different tokenizers and vocabularies, one for each language. The tf.keras framework provides a very powerful and versatile tokenizer class â€“ here we have set filters to an empty string and lower to False because we have already done what was needed for tokenization in our preprocess_sentence() function. The Tokenizer creates various data structures from which we can compute the vocabulary sizes and lookup tables that allow us to go from word to word index and back.\r\n",
        "\r\n",
        "Next we handle different length sequences of words by padding with zeros at the end, using the pad_sequences() function. Because our strings are fairly short, we do not do any truncation; we just pad to the maximum length of sentence that we have (8 words for English, and 16 words for French):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNL7T7gbY7oj",
        "outputId": "aa577831-6452-4bd5-e868-6837e1b1b4cd"
      },
      "source": [
        "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_en.fit_on_texts(sents_en)\r\n",
        "data_en = tokenizer_en.texts_to_sequences(sents_en)\r\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_en, padding=\"post\")\r\n",
        "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "    filters=\"\", lower=False)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_in)\r\n",
        "tokenizer_fr.fit_on_texts(sents_fr_out)\r\n",
        "data_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\r\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_fr_in, padding=\"post\")\r\n",
        "data_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\r\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "    data_fr_out, padding=\"post\")\r\n",
        "vocab_size_en = len(tokenizer_en.word_index)\r\n",
        "vocab_size_fr = len(tokenizer_fr.word_index)\r\n",
        "word2idx_en = tokenizer_en.word_index\r\n",
        "idx2word_en = {v:k for k, v in word2idx_en.items()}\r\n",
        "word2idx_fr = tokenizer_fr.word_index\r\n",
        "idx2word_fr = {v:k for k, v in word2idx_fr.items()}\r\n",
        "print(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\r\n",
        "    vocab_size_en, vocab_size_fr))\r\n",
        "maxlen_en = data_en.shape[1]\r\n",
        "maxlen_fr = data_fr_out.shape[1]\r\n",
        "print(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab size (en): 4359, vocab size (fr): 7590\n",
            "seqlen (en): 8, (fr): 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00o1kYPAKhQK"
      },
      "source": [
        "batch_size = 64\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\r\n",
        "    (data_en, data_fr_in, data_fr_out))\r\n",
        "dataset = dataset.shuffle(10000)\r\n",
        "test_size = NUM_SENT_PAIRS // 4\r\n",
        "test_dataset = dataset.take(test_size).batch(\r\n",
        "    batch_size, drop_remainder=True)\r\n",
        "train_dataset = dataset.skip(test_size).batch(\r\n",
        "    batch_size, drop_remainder=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhe0j7pvZ9-1"
      },
      "source": [
        "Our data is now ready to be used for training the seq2seq network, which we will define next. Our encoder is an Embedding layer followed by a GRU layer. The input to the encoder is a sequence of integers, which is converted to a sequence of embedding vectors of size embedding_dim. This sequence of vectors is sent to an RNN, which converts the input at each of the num_timesteps time steps to a vector of size encoder_dim. Only the output at the last time step is returned, as shown by the return_sequences=False."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfN9wFQmaWem"
      },
      "source": [
        "In our example network, we have chosen our embedding dimension to be 128, followed by the encoder and decoder RNN dimension of 1024 each. Note that we have to add 1 to the vocabulary size for both the English and French vocabularies to account for the PAD character that was added during the pad_sequences() step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku8yQJhyZ-VU"
      },
      "source": [
        "class Encoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps, \r\n",
        "            encoder_dim, **kwargs):\r\n",
        "        super(Encoder, self).__init__(**kwargs)\r\n",
        "        self.encoder_dim = encoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            encoder_dim, return_sequences=False, return_state=True)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, initial_state=state)\r\n",
        "        return x, state\r\n",
        "\r\n",
        "    def init_state(self, batch_size):\r\n",
        "        return tf.zeros((batch_size, self.encoder_dim))\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(tf.keras.Model):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, num_timesteps,\r\n",
        "            decoder_dim, **kwargs):\r\n",
        "        super(Decoder, self).__init__(**kwargs)\r\n",
        "        self.decoder_dim = decoder_dim\r\n",
        "        self.embedding = tf.keras.layers.Embedding(\r\n",
        "            vocab_size, embedding_dim, input_length=num_timesteps)\r\n",
        "        self.rnn = tf.keras.layers.GRU(\r\n",
        "            decoder_dim, return_sequences=True, return_state=True)\r\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\r\n",
        "\r\n",
        "    def call(self, x, state):\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, state = self.rnn(x, state)\r\n",
        "        x = self.dense(x)\r\n",
        "        return x, state"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXsrKSkW9c1l"
      },
      "source": [
        "embedding_dim = 256\r\n",
        "encoder_dim, decoder_dim = 1024, 1024\r\n",
        "encoder = Encoder(vocab_size_en+1, \r\n",
        "    embedding_dim, maxlen_en, encoder_dim)\r\n",
        "decoder = Decoder(vocab_size_fr+1, \r\n",
        "    embedding_dim, maxlen_fr, decoder_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iZO-vBxa0Sa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05c11ce-d42d-4761-eb0a-93c6f9f506e2"
      },
      "source": [
        "for encoder_in, decoder_in, decoder_out in train_dataset:\r\n",
        "   encoder_state = encoder.init_state(batch_size)\r\n",
        "   encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "   decoder_state = encoder_state\r\n",
        "   decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "   break\r\n",
        "print(\"encoder input          :\", encoder_in.shape)\r\n",
        "print(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\r\n",
        "print(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\r\n",
        "print(\"decoder output (labels):\", decoder_out.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoder input          : (64, 8)\n",
            "encoder output         : (64, 1024) state: (64, 1024)\n",
            "decoder output (logits): (64, 16, 7591) state: (64, 1024)\n",
            "decoder output (labels): (64, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4YnlNCX9mUE"
      },
      "source": [
        "This produces the following output, which is in line with our expectations. The encoder input is a batch of a sequence of integers, each sequence being of size 8, which is the maximum number of tokens in our English sentences, so its dimension is (batch_size, maxlen_en).\r\n",
        "\r\n",
        "The output of the encoder is a single tensor (return_sequences=False) of shape (batch_size, encoder_dim) and represents a batch of context vectors representing the input sentences. The encoder state tensor has the same dimensions. The decoder outputs are also a batch of sequence of integers, but the maximum size of a French sentence is 16; therefore, the dimensions are (batch_size, maxlen_fr). The decoder predictions are a batch of probability distributions across all time steps; hence the dimensions are (batch_size, maxlen_fr, vocab_size_fr+1), and the decoder state is the same dimension as the encoder state (batch_size, decoder_dim):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVV_clv0bcFe"
      },
      "source": [
        "Next we define the loss function. Because we padded our sentences, we don't want to bias our results by considering equality of pad words between the labels and predictions. Our loss function masks our predictions with the labels, so padded positions on the label are also removed from the predictions, and we only compute our loss using the non zero elements on both the label and predictions. This is done as follows:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPqiu4lubcaP"
      },
      "source": [
        "def loss_fn(ytrue, ypred):\r\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\r\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\r\n",
        "    loss = scce(ytrue, ypred, sample_weight=mask)\r\n",
        "    return loss\r\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfqbZBEDK0Ir"
      },
      "source": [
        "Because the seq2seq model is not easy to package into a simple Keras model, we have to handle the training loop manually as well. Our train_step() function handles the flow of data and computes the loss at each step, applies the gradient of the loss back to the trainable weights, and returns the loss.\r\n",
        "\r\n",
        "Notice that the training code is not quite the same as what was described in our discussion of the seq2seq model earlier. Here it appears that the entire decoder_input is fed in one go into the decoder to produce the output offset by one time step, whereas in the discussion, we said that this happens sequentially, where the token generated in the previous time step is used as the input to the next time step.\r\n",
        "\r\n",
        "This is a common technique used to train seq2seq networks, which is called Teacher Forcing, where the input to the decoder is the ground truth output instead of the prediction from the previous time step. This is preferred because it makes training faster, but also results in some degradation in prediction quality. To offset this, techniques such as Scheduled Sampling can be used, where the input is sampled randomly either from the ground truth or the prediction at the previous time step, based on some threshold (depends on the problem, but usually varies between 0.1 and 0.4):\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJecPJvrKHGD"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(encoder_in, decoder_in, decoder_out, encoder_state):\r\n",
        "   with tf.GradientTape() as tape:\r\n",
        "       encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "       decoder_state = encoder_state\r\n",
        "       decoder_pred, decoder_state = decoder(\r\n",
        "           decoder_in, decoder_state)\r\n",
        "       loss = loss_fn(decoder_out, decoder_pred)\r\n",
        "  \r\n",
        "   variables = (encoder.trainable_variables + \r\n",
        "       decoder.trainable_variables)\r\n",
        "   gradients = tape.gradient(loss, variables)\r\n",
        "   optimizer.apply_gradients(zip(gradients, variables))\r\n",
        "   return loss"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtDbYj23K5ie"
      },
      "source": [
        "The predict() method is used to randomly sample a single English sentence from the dataset and use the model trained so far to predict the French sentence. For reference, the label French sentence is also displayed. The evaluate() method computes the BiLingual Evaluation Understudy (BLEU) score [35] between the label and prediction across all records in the test set. BLEU scores are generally used where multiple ground truth labels exist (we have only one), but compares up to 4-grams (n-grams with n=4) in both reference and candidate sentences. Both the predict() and evaluate() methods are called at the end of every epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9I2b6VMKLMB"
      },
      "source": [
        "def predict(encoder, decoder, batch_size, \r\n",
        "        sents_en, data_en, sents_fr_out, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "    random_id = np.random.choice(len(sents_en))\r\n",
        "    print(\"input    : \",  \" \".join(sents_en[random_id]))\r\n",
        "    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\r\n",
        "\r\n",
        "    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\r\n",
        "    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\r\n",
        "\r\n",
        "    encoder_state = encoder.init_state(1)\r\n",
        "    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "    decoder_state = encoder_state\r\n",
        "\r\n",
        "    decoder_in = tf.expand_dims(\r\n",
        "        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\r\n",
        "    pred_sent_fr = []\r\n",
        "    while True:\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1)\r\n",
        "        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\r\n",
        "        pred_sent_fr.append(pred_word)\r\n",
        "        if pred_word == \"EOS\":\r\n",
        "            break\r\n",
        "        decoder_in = decoder_pred\r\n",
        "    \r\n",
        "    print(\"predicted: \", \" \".join(pred_sent_fr))\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoHxSW75-rKQ"
      },
      "source": [
        "def evaluate_bleu_score(encoder, decoder, test_dataset, \r\n",
        "        word2idx_fr, idx2word_fr):\r\n",
        "\r\n",
        "    bleu_scores = []\r\n",
        "    smooth_fn = SmoothingFunction()\r\n",
        "    for encoder_in, decoder_in, decoder_out in test_dataset:\r\n",
        "        encoder_state = encoder.init_state(batch_size)\r\n",
        "        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\r\n",
        "        decoder_state = encoder_state\r\n",
        "        decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\r\n",
        "\r\n",
        "        # compute argmax\r\n",
        "        decoder_out = decoder_out.numpy()\r\n",
        "        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\r\n",
        "\r\n",
        "        for i in range(decoder_out.shape[0]):\r\n",
        "            ref_sent = [idx2word_fr[j] for j in decoder_out[i].tolist() if j > 0]\r\n",
        "            hyp_sent = [idx2word_fr[j] for j in decoder_pred[i].tolist() if j > 0]\r\n",
        "            # remove trailing EOS\r\n",
        "            ref_sent = ref_sent[0:-1]\r\n",
        "            hyp_sent = hyp_sent[0:-1]\r\n",
        "            bleu_score = sentence_bleu([ref_sent], hyp_sent, \r\n",
        "                smoothing_function=smooth_fn.method1)\r\n",
        "            bleu_scores.append(bleu_score)\r\n",
        "\r\n",
        "    return np.mean(np.array(bleu_scores))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjlK5PqoK-Id"
      },
      "source": [
        "The training loop is shown as follows. We will use the Adam optimizer for our model. We also set up a checkpoint so we can save our model after every 10 epochs. We then train the model for 250 epochs, and print out the loss, an example sentence and its translation, and the BLEU score computed over the entire test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK97Y1OmK-he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361a3dba-657b-4f9e-f6de-373541e4768e"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\r\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\r\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\r\n",
        "                                encoder=encoder,\r\n",
        "                                decoder=decoder)\r\n",
        "num_epochs = 250\r\n",
        "eval_scores = []\r\n",
        "for e in range(num_epochs):\r\n",
        "   encoder_state = encoder.init_state(batch_size)\r\n",
        "   for batch, data in enumerate(train_dataset):\r\n",
        "       encoder_in, decoder_in, decoder_out = data\r\n",
        "       # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\r\n",
        "       loss = train_step(encoder_in, decoder_in, decoder_out, encoder_state)\r\n",
        "  \r\n",
        "   print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\r\n",
        "   if e % 10 == 0:\r\n",
        "       checkpoint.save(file_prefix=checkpoint_prefix)\r\n",
        "  \r\n",
        "   predict(encoder, decoder, batch_size, sents_en, data_en,\r\n",
        "       sents_fr_out, word2idx_fr, idx2word_fr)\r\n",
        "   eval_score = evaluate_bleu_score(encoder, decoder, \r\n",
        "       test_dataset, word2idx_fr, idx2word_fr)\r\n",
        "   print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\r\n",
        "   # eval_scores.append(eval_score)\r\n",
        "checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 1.1656\n",
            "input    :  what a strange man !\n",
            "label    :  quel homme bizarre ! EOS\n",
            "predicted:  quel est ton pantalon ! EOS\n",
            "Eval Score (BLEU): 2.360e-02\n",
            "Epoch: 2, Loss: 0.8047\n",
            "input    :  i m a truck driver .\n",
            "label    :  je suis un chauffeur de camion . EOS\n",
            "predicted:  je suis un nouvel gars . EOS\n",
            "Eval Score (BLEU): 3.503e-02\n",
            "Epoch: 3, Loss: 0.6707\n",
            "input    :  she looks confused .\n",
            "label    :  elle a l air desorientee . EOS\n",
            "predicted:  elle a l air perplexe . EOS\n",
            "Eval Score (BLEU): 4.671e-02\n",
            "Epoch: 4, Loss: 0.4774\n",
            "input    :  get on a horse .\n",
            "label    :  enfourche un cheval ! EOS\n",
            "predicted:  prends un peu de ceux ci ! EOS\n",
            "Eval Score (BLEU): 6.051e-02\n",
            "Epoch: 5, Loss: 0.4084\n",
            "input    :  i was careless .\n",
            "label    :  j etais insouciante . EOS\n",
            "predicted:  j etais submerge . EOS\n",
            "Eval Score (BLEU): 7.898e-02\n",
            "Epoch: 6, Loss: 0.3073\n",
            "input    :  we need proof .\n",
            "label    :  il nous faut des preuves . EOS\n",
            "predicted:  nous avons besoin de plus . EOS\n",
            "Eval Score (BLEU): 9.364e-02\n",
            "Epoch: 7, Loss: 0.2783\n",
            "input    :  what s it all mean ?\n",
            "label    :  que signifie tout ceci ? EOS\n",
            "predicted:  comment va tout ceci ? EOS\n",
            "Eval Score (BLEU): 1.117e-01\n",
            "Epoch: 8, Loss: 0.2382\n",
            "input    :  i stood .\n",
            "label    :  je me suis tenue debout . EOS\n",
            "predicted:  je me suis tenu debout . EOS\n",
            "Eval Score (BLEU): 1.238e-01\n",
            "Epoch: 9, Loss: 0.1696\n",
            "input    :  we re all retired .\n",
            "label    :  nous sommes tous retraites . EOS\n",
            "predicted:  nous sommes toutes retraitees . EOS\n",
            "Eval Score (BLEU): 1.328e-01\n",
            "Epoch: 10, Loss: 0.1946\n",
            "input    :  that was amazing .\n",
            "label    :  c etait incroyable . EOS\n",
            "predicted:  c etait incroyable . EOS\n",
            "Eval Score (BLEU): 1.415e-01\n",
            "Epoch: 11, Loss: 0.1529\n",
            "input    :  was it necessary ?\n",
            "label    :  etait ce necessaire ? EOS\n",
            "predicted:  etait ce necessaire ? EOS\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}